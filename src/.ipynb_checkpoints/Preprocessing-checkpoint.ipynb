{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26123009",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c073d375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler, LabelEncoder\n",
    "import pickle\n",
    "\n",
    "# get cleaned data\n",
    "file = open('../data/data_cleaned.save', 'rb')\n",
    "df, X, y, subject_id = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "\n",
    "# create preprocessor\n",
    "ordinal_ftrs = ['EDUC']\n",
    "ordinal_cats = [list(range(1, 24))]\n",
    "\n",
    "onehot_ftrs = ['Gender', 'SES']\n",
    "minmax_ftrs = ['Age', 'MMSE', 'nWBV']\n",
    "std_ftrs = ['eTIV', 'ASF']\n",
    "\n",
    "# collect all the encoders\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ord', OrdinalEncoder(categories = ordinal_cats), ordinal_ftrs),\n",
    "        ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'), onehot_ftrs),\n",
    "        ('minmax', MinMaxScaler(), minmax_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "\n",
    "\n",
    "file = open('../data/preprocessor.save', 'wb')\n",
    "pickle.dump((preprocessor),file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0585163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    31\n",
      "0.5    30\n",
      "1.0    11\n",
      "Name: CDR, dtype: int64\n",
      "0.0    175\n",
      "0.5     93\n",
      "1.0     28\n",
      "Name: CDR, dtype: int64\n",
      "train:\n",
      " 0.0    122\n",
      "0.5     77\n",
      "1.0     23\n",
      "Name: CDR, dtype: int64\n",
      "val:\n",
      " 0.0    44\n",
      "0.5    24\n",
      "1.0     6\n",
      "Name: CDR, dtype: int64\n",
      "train:\n",
      " 0.0    127\n",
      "0.5     76\n",
      "1.0     19\n",
      "Name: CDR, dtype: int64\n",
      "val:\n",
      " 0.0    39\n",
      "0.5    25\n",
      "1.0    10\n",
      "Name: CDR, dtype: int64\n",
      "train:\n",
      " 0.0    133\n",
      "0.5     68\n",
      "1.0     21\n",
      "Name: CDR, dtype: int64\n",
      "val:\n",
      " 0.5    33\n",
      "0.0    33\n",
      "1.0     8\n",
      "Name: CDR, dtype: int64\n",
      "train:\n",
      " 0.0    116\n",
      "0.5     82\n",
      "1.0     24\n",
      "Name: CDR, dtype: int64\n",
      "val:\n",
      " 0.0    50\n",
      "0.5    19\n",
      "1.0     5\n",
      "Name: CDR, dtype: int64\n",
      "fraction of missing values in features:\n",
      "Series([], dtype: float64)\n",
      "fraction of points with missing values: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold#, StratifiedGroupKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "random_state = 5\n",
    "\n",
    "# create split objects for initial GroupShuffleSplit and subsequent GroupKFold, random state to ensure reproducibility\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=.2, random_state=random_state)\n",
    "group_kfold_train = GroupKFold(n_splits=4)\n",
    "\n",
    "\n",
    "# separate test set from other sets    \n",
    "for i_other,i_test in gss.split(X, y, subject_id):\n",
    "    X_other, y_other, groups_other = X.iloc[i_other], y.iloc[i_other], subject_id.iloc[i_other]\n",
    "    X_test, y_test, groups_test = X.iloc[i_test], y.iloc[i_test], subject_id.iloc[i_test]\n",
    "\n",
    "# check to see if each target class is represented in test\n",
    "print(y_test.value_counts())\n",
    "print(y_other.value_counts())\n",
    "\n",
    "# do GroupKFolds split for training and validation data\n",
    "for train_idx, val_idx in group_kfold_train.split(X_other, y_other, groups_other):\n",
    "    X_train = X.iloc[train_idx, :]\n",
    "    X_val = X.iloc[val_idx, :]\n",
    "    y_train = y.iloc[train_idx]\n",
    "    y_val = y.iloc[val_idx]\n",
    "\n",
    "    # check to see if each target class is represented in train and val\n",
    "    print(\"train:\\n\", y_train.value_counts())\n",
    "    print(\"val:\\n\", y_val.value_counts())\n",
    "\n",
    "    # preprocess the data\n",
    "    clf = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "    X_train_prep = clf.fit_transform(X_train)\n",
    "    X_val_prep = clf.transform(X_val)\n",
    "    X_test_prep = clf.transform(X_test)\n",
    "\n",
    "#         #check shapes\n",
    "#         print(X_train.shape)\n",
    "#         print(X_train_prep.shape)\n",
    "#         display(pd.DataFrame(X_train))\n",
    "#         display(pd.DataFrame(X_train_prep))\n",
    "\n",
    "\n",
    "#     file = open('../data/data_preprocessed.save', 'wb')\n",
    "#     pickle.dump((X_train_prep, X_val_prep, X_test_prep, y_train, y_val, y_test),file)\n",
    "#     file.close()\n",
    "\n",
    "# # check distribution of data into sets\n",
    "# print(\"train proportion: \", X_train_prep.shape[0]/X.shape[0])\n",
    "# print(\"val proportion: \", X_val_prep.shape[0]/X.shape[0])\n",
    "# print(\"test proportion: \", X_test_prep.shape[0]/X.shape[0])\n",
    "\n",
    "df_train = pd.DataFrame(data = X_train_prep)\n",
    "df_val = pd.DataFrame(data = X_val_prep)\n",
    "\n",
    "# frac_missing = sum(df_train.isnull().sum(axis=1)!=0)/df_train.shape[0]\n",
    "# frac_missing\n",
    "\n",
    "# examine how much data is missing from the dataset\n",
    "perc_missing_per_ftr = df_val.isnull().sum(axis=0)/df_val.shape[0]\n",
    "print('fraction of missing values in features:')\n",
    "print(perc_missing_per_ftr[perc_missing_per_ftr > 0])\n",
    "frac_missing = sum(df_val.isnull().sum(axis=1)!=0)/df_val.shape[0]\n",
    "print('fraction of points with missing values:',frac_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86be1b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=2000, random_state=5, solver='saga')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_prep = le.fit_transform(y_train)\n",
    "\n",
    "log_reg = LogisticRegression(solver='saga', random_state=5, max_iter = 2000)\n",
    "\n",
    "log_reg.fit(X_train_prep, y_train_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3ff051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
